# Load Test Results - January 16, 2024

## Test Configuration

- **Tool:** k6 v0.48.0
- **Duration:** 3.5 minutes
- **Target Load:** 20 concurrent VUs (reduced from initial 50 after system crashed)
- **Test Environment:** Local laptop (16GB RAM), Docker Compose

## What Happened (The Honest Version)

First attempt with 50 VUs completely crashed the system after 45 seconds. PostgreSQL connection pool exhausted, Kafka consumer lagged by 30+ seconds, and the outbox publisher couldn't keep up. Had to kill everything and restart.

Second attempt with 30 VUs was better but still saw deadlocks around the 2-minute mark. Database was throwing "could not obtain lock" errors.

Third attempt with 20 VUs finally worked, but still saw some issues.

## Results Summary

```
     ✓ status is 200 or 409
     ✓ has transactionId

     checks.........................: 99.12% ✓ 4234       ✗ 38    
     data_received..................: 1.2 MB  5.7 kB/s
     data_sent......................: 2.8 MB  13 kB/s
     http_req_blocked...............: avg=18.45µs  min=3µs     med=9µs      max=3.2ms    p(90)=22µs    p(95)=34µs   
     http_req_connecting............: avg=5.12µs   min=0s      med=0s       max=2.1ms    p(90)=0s      p(95)=0s     
     http_req_duration..............: avg=423.67ms min=45.23ms med=312.45ms max=5.2s     p(90)=876ms   p(95)=1.2s   
       { expected_response:true }...: avg=421.34ms min=45.23ms med=310.12ms max=5.2s     p(90)=872ms   p(95)=1.18s  
     http_req_failed................: 0.88%   ✓ 38         ✗ 4196
     http_req_receiving.............: avg=112.34µs min=28µs    med=84µs     max=4.1ms    p(90)=187µs   p(95)=245µs  
     http_req_sending...............: avg=67.23µs  min=18µs    med=48µs     max=2.3ms    p(90)=112µs   p(95)=156µs  
     http_req_tls_handshaking.......: avg=0s       min=0s      med=0s       max=0s       p(90)=0s      p(95)=0s     
     http_req_waiting...............: avg=423.49ms min=45.11ms med=312.28ms max=5.2s     p(90)=875ms   p(95)=1.2s   
     http_reqs......................: 4234    20.16/s
     iteration_duration.............: avg=923.67ms min=545ms   med=812ms    max=5.7s     p(90)=1.38s   p(95)=1.72s  
     iterations.....................: 4234    20.16/s
     vus............................: 1       min=1        max=20  
     vus_max........................: 20      min=20       max=20  

Custom Metrics:
     duplicates.....................: 0.24%   (10 duplicates detected)
     errors.........................: 0.88%   (38 errors)
     latency_ms.....................: avg=423ms p(95)=1200ms p(99)=2100ms
```

## Analysis

### What Worked

1. **Idempotency Detection: 0.24%**
   - 10 duplicate requests detected and handled correctly
   - Returned cached results without double-processing
   - Proves idempotency logic works

2. **Throughput: 20 TPS**
   - Way below target 2,000 TPS
   - But stable at 20 TPS for 3 minutes
   - No crashes or data loss

3. **Error Rate: 0.88%**
   - 38 errors out of 4,234 requests
   - Most errors were deadlocks (see below)
   - Acceptable for a load test

### What Didn't Work

1. **p95 Latency: 1.2s**
   - ❌ Misses target SLA of p95 < 500ms (by a lot)
   - Revised target: p95 < 1000ms (still missed)
   - Pessimistic locking is slow

2. **p99 Latency: 2.1s**
   - ❌ Way over target
   - Some requests took 5+ seconds
   - Database contention is the issue

3. **Deadlocks: 38 occurrences**
   - PostgreSQL threw "deadlock detected" errors
   - Happens when two transactions try to lock the same accounts in different orders
   - Added retry logic, but it's a band-aid

4. **Outbox Publisher Lag**
   - Outbox table grew to 500+ pending events
   - Publisher couldn't keep up with transaction rate
   - Events were delayed by 5-10 seconds

### Bottlenecks Identified

1. **Pessimistic Locking**
   - SELECT FOR UPDATE locks entire row
   - Causes contention when multiple transactions touch same account
   - Trade-off: Correctness vs performance

2. **Database Connection Pool**
   - Started with 10 connections, increased to 30
   - Still saw "connection pool exhausted" warnings
   - Need connection pooling strategy

3. **Outbox Publisher Single Thread**
   - Only one thread publishing events
   - Can't keep up with 20 TPS
   - Need to parallelize or batch

4. **Kafka Offset Commits**
   - Manual offset management adds latency
   - Each transaction waits for offset commit
   - Could batch offset commits

## Failed Experiments

1. **Optimistic Locking**
   - Tried replacing pessimistic with optimistic locking
   - Result: 30% of transactions failed with "version mismatch"
   - Rolled back to pessimistic locking

2. **Batching Transactions**
   - Tried processing 10 transactions in one database transaction
   - Result: Deadlocks increased to 15%
   - Rolled back to one-at-a-time

3. **Async Outbox Publishing**
   - Tried making outbox publishing fully async (fire-and-forget)
   - Result: Lost events when system crashed
   - Rolled back to synchronous

## Recommendations

1. **Shard by Account ID**
   - Route transactions to different database shards based on account ID
   - Would reduce contention
   - Tradeoff: Complexity

2. **Batch Outbox Publishing**
   - Publish 100 events at once instead of one-at-a-time
   - Would reduce Kafka overhead
   - Tradeoff: Latency

3. **Read Replicas for Idempotency Checks**
   - Check idempotency on replica instead of primary
   - Would reduce load on primary
   - Tradeoff: Replication lag could miss duplicates

4. **Revise SLA Targets**
   - p95 < 500ms is not achievable with pessimistic locking
   - More realistic: p95 < 1500ms, p99 < 3000ms

## Conclusion

The system works correctly (no data loss, idempotency works) but performance is not great. The main issue is database contention from pessimistic locking. This is a fundamental tradeoff: correctness vs performance.

For a financial system, correctness is more important than performance. But if we need higher throughput, we'd need to rethink the architecture (sharding, optimistic locking with retries, or event sourcing).

**Key Takeaway:** Distributed transactions are hard. Pessimistic locking guarantees correctness but kills performance. There's no free lunch.
